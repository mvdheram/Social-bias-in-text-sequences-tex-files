\chapter{Experiments}

This chapter discusses the experiment performed with different models to classify stereotypical social biases using the stereotypical dataset discussed in the previous section. Section \ref{task_overview} describes the basic overview of the classification task, with some insights into the data. Section \ref{experimental_setup}  discusses the basic setup for performing experiments. Section \ref{assessing_social_biases} describes the language models and baselines used, along with the training procedure. Finally, \ref{Evaluation_metrics} describes the evaluation metrics used when assessing the results.

\section{ Task overview}\label{task_overview}
The main goal of the thesis is to assess social stereotypes present in the text sequences using language models. The reasons for using language models which are based on transfer learning approach are as follows. Firstly, when training a model using the classic supervised learning approach, wherein a model is trained on a dataset belonging to the source domain with training samples that demonstrate correct behavior and then tested on identically distributed held out samples, the model tends to perform well \cite{ruder2019neural}. Moreover, these models require several hundred to thousands of samples to induce functions that generalize well \cite{radford2019language}. This approach might not suit the task of assessing social stereotypes as the number of samples is relatively less. Transfer learning approach, on the other hand, is more preferable for the current setting wherein the knowledge gained through training on the source domain/task is stored as representations (vectors) and applied to the target domain, or task \cite{ruder2019neural} rather than training from scratch as done in classic supervised learning approach. Transfer learning in the form of pre-trained language models has become ubiquitous in natural language processing and contributed to state of the art in a wide range of tasks\cite{ruder2019transfer}. Since the pre-trained language models have been trained on large amounts of real-world data, the need for annotated data is reduced by 10x. Transfer learning using pre-trained language models has been shown to achieve similar performance compared to a non-pre-trained model with 10x fewer samples \cite{howard2018universal}. During this pre-training, they also capture the stereotypical biases present in the data \cite{nadeem2020stereoset}. This knowledge can thus be used for the target task of classifying stereotypical social biases. Considering these reasons, experiments have been performed on different pre-trained language models. In order to evaluate the performance of language models and to answer the second research question of how far transfer learning can be used to detect stereotypes, baselines have been trained, which will be discussed in the corresponding section.

Considering the task of classification, several experiments using different combinations of labels (stereotype, anti-stereotype, different bias types) and different classification tasks (binary, multi-class, multi-label) have been performed to test the different perspectives of predictions. Finally, multi-label classification was chosen with ethnicity, gender, profession, religion, stereotype, anti-stereotype, unrelated as labels. Considering the datasets StereoSet and CrowS-Pairs, as can be seen in the table \ref{tab:stereoset} and \ref{tab:Crowspair}, for each bias type, there is a stereotypical as well as anti-stereotypical instance. Considering the perspective that stereotypes only have meaning to the extent they are socially shared \cite{macrae1996stereotypes}, anti-stereotype samples give the perspective that the sample is a stereotype but not socially shared. Hence, considering anti-stereotype along with stereotype as a label gives the perspective whether a sample is socially shared or not socially shared stereotype. To put it more clearly, stereotype samples are socially shared stereotypes, while anti-stereotypes are not socially shared stereotypes. StereoSet dataset also contains an unrelated instance per sample. This instance can be used as a sample that is neither socially shared (stereotype) nor not socially shared (anti-stereotype) but a totally unrelated sample. Hence, to the pre-processed dataset \ref{tab:Bias_type_stats_preprocessing}, anti-stereotypical samples from StereoSet (combining context and anti\_stereotype instance ; see table \ref{tab:stereoset} for reference) and CrowS-Pairs(sent\_less of crowspair; see table \ref{tab:Crowspair} for reference) are added. StereoSet Unrelated intersentence samples are added, excluding intrasentence unrelated. Considering the intrasentence stated in the table \ref{tab:stereoset}, the unrelated instance would be "girls tend to be more \textit{fish} than boys," which does not make any sense and thus are omitted. Data augmentation technique is used to generate unrelated synthetic samples using NLPaug \footnote{\url{https://github.com/makcedward/nlpaug}} library. The unrelated samples of intersentence were used as a reference to generate synthetic samples by substituting a certain number of words (word augmenter). Contextual word embeddings of the RoBERTa-base model are used to substitute the original word in the reference sentence using similarity measures between the original word and the word embeddings of the model. Hence, the final dataset statistics after adding anti-stereotypical samples and unrelated samples are as shown in the figure \ref{tab:data_final_stats}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[]
\centering
\scalebox{0.3}{
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lr@{}}
\toprule
Biat type             & No. of samples             \\ \midrule
Ethnicity             & 5226                       \\
Profession            & 3112                       \\
Gender                & 2024                       \\
Religion              & 1953                       \\\bottomrule
\addlinespace
Total                 & 16560 \\ \bottomrule
% \multicolumn{1}{l|}{Total} & \multicolumn{1}{|l}{16560} \\ \bottomrule
\end{tabular}%
}}
\caption{Data statistics after combining anti-stereotypical and unrelated samples}
\label{tab:data_final_stats}
\end{table}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllllll@{}}
\toprule
Ethnicity & Gender & Profession & Religion & Anti-stereotype & Stereotype & Unrelated & Sentence                                                         \\ \midrule
1 & 0 & 0 & 0 & 0 & 1 & 0 & Many people live in Ethiopia. The people are very thin and good at distance running \\
1         & 0      & 0          & 0        & 1               & 0          & 0         & Many people live in Ethiopia. The people are fat and unatheletic \\
0         & 0      & 0          & 0        & 0               & 0          & 1         & I saw a cowboy                                                   \\ \bottomrule
\end{tabular}%
}
\caption{One-hot encoded stereotypical dataset }
\label{tab:ohe_df}
\end{table}

As shown in the table \ref{tab:ohe_df}, the final dataset consists of One-hot encoded vectors where for each sample of stereotype (socially shared) or anti-stereotype (not socially shared), a corresponding bias type i.e. either ethnicity, gender, profession, or religion is also present. This categorization thus answers whether a sample is a socially shared stereotype or not along with the bias type or it is a totally unrelated sample.
% \begin{itemize}
%     \item Why transfer learning-based approach ?
%     \begin{itemize}
%         \item Less number of samples available due to the multidimensional nature of stereotypes, and collecting and validating stereotypes is not a straight forward task.  
%         \item Use of the real world knowledge of language model to help felicitate the inadequacy. 
%     \end{itemize}
%     \item Why classification ?
%     \item Final goal of assessing stereotypical social bias in arguments produced by args.me search engine
% \end{itemize}

% Steps: 
% \begin{enumerate}
%     \item Collecting stereotypical samples with respect to different domains such as race, gender, profession etc. are gathered from the available sources (CrowS-Pairs \cite{nangia2020crows}, Social bias frames \cite{sap2019social}, StereoSet \cite{nadeem2020stereoset} as seen so far).
%     \item Exploratory data analysis for some insights into the different types of biases (gender, race, profession, religion).
%     \item Stereotypical dataset target labels interpretation
%         \begin{itemize}
%             \item Different perspectives of classification labels used 
%             \item Anti-stereotypical samples introduced to get the perspective whether a text sequence is socially shared or not socially shared, as stereotypes can be interpreted as pictures in head.
%             \item  Including unrelated as a category in classification, why?
%         \end{itemize}
%     \item Categorization experiments
%     \begin{itemize}
%         \item Binary classification 
%         \item Multi-class classification 
%         \item Multi-label classification 
%         \item Why going with multi-label ?? When considering anti-stereotypical 
%     \end{itemize}
% \end{enumerate}
\section{Experimental setup}
\label{experimental_setup}
All the experiments including language models and baselines in this thesis are performed using Google Colaboratory \footnote{\url{https://colab.research.google.com/}}. For training language models, one NVIDIA Tesla T4 GPU provided by google Colaboratory is used. Baseline models have also been trained using google colab on 12GB RAM. For training language models mainly three libraries were used hugging face transformers \footnote{\url{https://huggingface.co/transformers/}}, Ktrain \footnote{\url{https://github.com/amaiya/ktrain}} and PyTorch lightening \footnote{\url{https://www.pytorchlightning.ai/}}. RayTune \footnote{\url{https://docs.ray.io/en/latest/tune/index.html}} was used to perform hyperparameter search for the language models. Keras \footnote{\url{https://keras.io/}} and scikit learn \footnote{\url{https://scikit-learn.org/stable/}} libraries were used to train baseline models.

\section{Assessing stereotypical social bias} \label{assessing_social_biases}
As discussed in the previous section, a multi-label classification task was chosen to assess stereotypical social biases present in text sequences. Considering research question 1, four different pre-trained language models have been selected to experiment, namely BERT \cite{devlin2018bert}, RoBERTa, \cite{liu2019roberta}, XLNet \cite{yang2019xlnet} and GPT-2  \cite{radford2019language}. The reason for choosing these models for experimenting is, the authors of SteroSet \cite{nadeem2020stereoset}, and CrowS-Pairs\cite{nangia2020crows} have used their datasets to assess the stereotypes encoded in these language models themselves, thus uncovering the different types of biases encoded in different language models. This information could be useful when interpreting the results after training the fine-tuned language models. In order to answer the second question, baseline models have been trained to evaluate the use of transfer learning. Devlin\cite{devlin2018bert} states that there are two approaches for applying the language representation, namely, feature-based approach where the language representation are applied as features to initialize the model; fine-tuning-based approach where the model itself is fine-tuned and trained end-to-end. In the following subsections, first, fine-tuning-based approach using language models is described with the training procedure. The next subsection discusses the feature-based approach trained on machine learning models from scratch by using different pre-trained embedding models and selected features. 
\subsection{Fine-tuning based approach}
As discussed in the previous section, four language models were selected and fine-tuned to the downstream task of multi-label text classification using the stereotypical dataset. The training pipeline followed in the fine-tuning-based approach can be briefly divided into five steps, as shown in the figure \footnote{\url{https://www.flaticon.com/}} \ref{fig:fine-tuning based approach}. Firstly, in the data pre-processing, the labels of stereotypical dataset need to be transformed into one-hot-encoded vectors for multi-label classification. In a multi-label classification setup, each sample belongs to several non-overlapping classes \cite{sokolova2009systematic}. Formally, multi-label learning can be defined as a learn function \textit{h}:X \rightarrow 2$^y$ from multi-label training set $D$ = $\{(x_i,y_i)| 1 \leq i\leq m\}$; where  $x_i \in X$ denotes d - dimensional instance space or feature vector and $y_i \in y$ denotes the  set of labels associated with $X_i$ \cite{zhang2013review}. The popular method when dealing with this problem is to create a binary classifier for each label \cite{ji2008extracting}, where a binary output, 1 - for positive classes relevant to the sample and 0 for the rest is assigned to each class. This transformation method is known as binary relevance \cite{zhang2010multi}. 
It is like having m independent binary classifiers for m labels. "When predicting on an unseen instance x, binary relevance predicts associated multi-label set y, by querying labeling relevance on individual binary classifiers and then combining the relevant labels"\cite{zhang2010multi}.  
% \pagebreak

\begin{figure}[]
    \centering
    \includegraphics[width=0.7\textwidth]{thesis/figures/LM pipeline.png}
    \caption{fine-tuning based approach pipeline}
    \label{fig:fine-tuning based approach}
\end{figure}

Hence, the labels of the stereotypical data are turned into one-hot encoded vectors as shown in the figure \ref{tab:ohe_df}. The one-hot encoded vectors are split into train, validation, and test sets. Stratified train test splits has been used to split the data into 70\% for training and 15\% for validation and test sets. Stratified sampling is helpful in preserving label/class proportion, or composition \cite{merrillees2021stratified}. Next, the input data is to be processed according to the corresponding language model specifications, which is when the samples can be fed for training. This is done using hugging face auto tokenizer \footnote{\url{https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoTokenizer}} class which consists of pre-trained tokenizers to encode the input data according to the format the language model accepts. The basic steps done by auto tokenizer are as follows, split the input sentences into tokens, add special tokens corresponding to the language model ( Considering BERT language model \cite{devlin2018bert}, tokens \textit{[CLS]} is added as the first token to each input sentence to indicate the start of the sequence and \textit{[SEP]} added between two sentences as a separator for sentence pair tasks). Next, 'input\_ids', 'token\_type\_ids' and 'attention\_mask' corresponding to tokens of input sentence are generated. The input ids are based on vocabulary (look-up table) generated during the pre-training of the language model. Token type ids are used to indicate two pairs of sentences for sentence pair tasks, and attention masks are used to indicate which tokens the model should pay attention to.  To encode batches of input sentences and take advantage of GPU parallel processing, the input sentences with varying lengths need to be batched with fixed lengths. For this, the sentences should be padded to maximum length per batch, and if the sentence length exceeds the maximum length, they need to be truncated. For our task, tokenizer.encode\_plus \footnote{\url{https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus}} is used to encode the text sequences with maximum length set to 50, and the best batch size for the corresponding model is determined while doing the hyperparameter search. This returns a dictionary of input\_ids, attention\_mask, labels. 

The next step is fine-tuning the language model to the target task, i.e. multi-label classification. For this, hugging face transformers auto model \footnote{\url{https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoModel}} has been used. This loads the corresponding language model with pre-trained weights. For our downstream task of classification, a linear layer is added to the loaded model. Firstly, the language model's outputs, which carry the contextual information learned, is to be selected/pooled as per the language model(generally, the last token of the last hidden layer). Considering the running example BERT, the final hidden vector corresponding to $[CLS]$ token is used as input to the linear layer as prescribed by authors\cite{devlin2018bert}. Hence, a linear layer with input size that of the last hidden unit corresponding to $[CLS]$ token and output size corresponding to the number of labels is created; in our case, 7 is created. Considering BERT, the linear layer shape should be [768,7].  Finally, the sigmoid activation function along with binary cross-entropy loss function is used. The sigmoid activation function converts raw outputs of the neural network into non-exclusive probabilities over classes and is hence used for multi-label classification. Since multi-label classification can be seen as multiple binary classifications, a binary cross-entropy loss function is applied for each class, and the loss is summed up to get an overall loss \textbf{needs to be cited<----}. This completes the fine-tuning step.

Coming to the next step, hyperparameter search of the fine-tuned language model is being done with the help of RayTune \footnote{\url{https://docs.ray.io/en/latest/tune/index.html}} (a python library for experiment execution and hyperparameter tuning). Model hyperparameters can be defined as the parameters used to control the learning process, which is set before training a model \cite{bergstra2012random} and the goal is to find optimal configuration parameters. The important steps for doing hyperparameter search are to define search space, search algorithm, and a metric to evaluate the model with hyperparameters. The search space used is defined in the table \ref{tab:search_space}. Uniform method of ray tune samples a float value uniformly between the mentioned upper and lower limits. Choice method of ray tune samples a categorical value in the provided range\footnote{\url{https://docs.ray.io/en/latest/tune/api_docs/search_space.html}}. The search algorithm used is basicVarientGenerator \footnote{\url{https://docs.ray.io/en/releases-0.8.5/tune-searchalg.html#variant-generation-grid-search-random-search}} of ray, which uses random search and grid search as algorithms. The objective metric is to reduce the validation loss. All the models are run for 5 trials with the following parameters as shown in the table \ref{tab:search_trials} and the trial that produces minimum validation loss is chosen as the best hyperparameters for the model.  The hyperparameters obtained after running through the 5 trails are compiled in the following table \ref{tab:search_results}. Since a common search space search algorithm and same number of trials are used; this forms a common base for all the models for a fair comparison.
% \begin{table}[]
% \centering
% \scalebox{0.5}{
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}llrrrr@{}}
% \toprule
%   & model\_name       & learning\_rate        & num\_train\_epochs & seed & per\_device\_train\_batch\_size \\ \midrule
% 0 & roberta-base      & 3.404460046972836e-05 & 5                  & 22   & 8                               \\
% 1 & xlnet-base-cased  & 2.49816047538945e-05  & 2                  & 15   & 32                              \\
% 2 & bert-base-uncased & 2.49816047538945e-05  & 2                  & 15   & 32                              \\
% 3 & gpt2              & 2.49816047538945e-05  & 2                  & 15   & 32                              \\ \bottomrule
% \end{tabular}%
% }}
% \caption{Hyperparameter search results for language models}
% \label{tab:search_results}
% \end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[]
\centering
\scalebox{0.7}{
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
Hyperparameter                  & Search space               \\ \midrule
learning\_rate                  & tune.uniform(1e-5,5e-5)    \\
num\_train\_epochs              & tune.choice({[}2,3,5{]})   \\
seed                            & tune.choice(range(1,41))   \\
per\_device\_train\_batch\_size & tune.choice({[}8,16,32{]}) \\ \bottomrule
\end{tabular}%
}}
\caption{Hyperparameter search space}
\label{tab:search_space}
\end{table}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rrrrr@{}}
\toprule
Trails & learning\_rate & num\_train\_epochs & per\_device\_train\_batch\_size & seed \\ \midrule
1           & 2.49816e-05    & 2                  & 32                              & 15   \\
2           & 4.11876e-05    & 2                  & 16                              & 39   \\
3           & 1.62398e-05    & 5                  & 8                               & 11   \\
4           & 3.40446r-05    & 5                  & 8                               & 22   \\
5           & 4.87964e-05    & 3                  & 16                              & 38   \\ \bottomrule
\end{tabular}%
}
\caption{Hyperparameter Search trails}
\label{tab:search_trials}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[h!]
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llrrrr@{}}
\toprule
  & model\_name       & learning\_rate        & num\_train\_epochs & seed & per\_device\_train\_batch\_size \\ \midrule
0 & roberta-base      & 3.404460046972836e-05 & 5                  & 22   & 8                               \\
1 & xlnet-base-cased  & 2.49816047538945e-05  & 2                  & 15   & 32                              \\
2 & bert-base-uncased & 2.49816047538945e-05  & 2                  & 15   & 32                              \\
3 & gpt2              & 2.49816047538945e-05  & 2                  & 15   & 32                              \\ \bottomrule
\end{tabular}%
}
\caption{Hyperparameter search results for language models}
\label{tab:search_results}
\end{table}

The language models are trained using these hyperparameters. Here, the training of the fine-tuned model is done end-to-end as done by the authors of BERT while fine-tuning on different Natural language processing tasks \cite{devlin2018bert}. The procedure remains the same across all the language models. The base versions (base model size) of the language models were used while training. It has been shown that very few samples (<100 samples) suffice when training on large (model size) language models with the goal to improve language model behavior with respect to sensitive attributes \cite{solaiman2021process}. 

Finally, all the trained models are evaluated on the same test split for a fair comparison. The evaluation metrics used will be discussed in a further section.


\subsubsection{Brief description of language models}

As described in the previous section, four language models have been used for experimenting. Broadly, language models can be divided into autoencoding and autoregressive models \cite{yang2019xlnet}. Auto encoding models are models corresponding to the encoder side of the original transformer \cite{vaswani2017attention} where the language models are pre-trained by aiming to reconstruct the original data from the corrupted input data \cite{yang2019xlnet}. Autoregressive models, on the other hand, correspond to the decoder side of original transformers, where the model is trained on the classic language modeling task of estimating the probability distribution of a text corpus \cite{yang2019xlnet}. BERT and RoBERTa are purely autoencoding models, GPT-2 is an autoregressive model, and XLNet is a generalized autoregressive model that builds on the traditional autoregressive model. Some characteristic features of these language models are briefly covered in this section. StereoSet authors have devised a metric to measure stereotypes encoded in the language models known as  Stereotypical score. This metric is used to test whether the language model prefers stereotypical association over anti-stereotypical association per target term \cite{nadeem2020stereoset} and the overall stereotypical score is the average stereotypical score over target terms in the dataset. Ideal stereotypical score would be 50 \cite{nadeem2020stereoset}. The stereotype score obtained by the language models is also covered.   

\subsubsection{BERT-base-uncased}

BERT, short for Bidirectional Encoder Representation from Transformers, is a multi-layer bidirectional encoder-based transformer language model\cite{devlin2018bert}. BERT is pre-trained on "masked language modeling"(MLM) and "next sentence predictions"(NSP). The masked language modeling task is a task wherein tokens in input data are randomly masked using $[MASK]$ token, and the objective is to predict these masked tokens based on their context or remaining tokens \cite{devlin2018bert}. This task enables the representations(word vectors) to capture both left and right contexts (bidirectional) and thus build more meaningful contextual representations of words. BERT was pre-trained on BookCorpus (11,038 unpublished books, 800 million words) and English Wikipedia (excluding lists, tables, and headers, 2,500 million words). BERT has an overall stereotype score of 59.6 (Ideal 50) with a higher presence of profession and gender stereotypes based on a study by \cite{nadeem2020stereoset}. 

\subsubsection{RoBERTa - base}
RoBERTa, short for Robustly optimized BERT approach, is an optimized version of BERT language model \cite{liu2019roberta}. Modifications that have been made on BERT are, removing the next sentence prediction (NSP), training on more data and using bigger batches while training, training on longer sequences, and dynamically changing the masking pattern applied to training data\cite{liu2019roberta}.  Roberta was trained on 5 datasets; namely, BookCorpus (11,038 unpublished books, 800 million words) and English Wikipedia (excluding lists, tables, and headers, 2,500 million words) as used by BERT and CC-News (English portion of Common crawl news dataset, 76 GB), OpenWebText (web content extracted from URLs shared on Reddit with at least three upvotes. 38GB), Stories ("subset of CommonCrawl data filtered to match story like style")\cite{liu2019roberta}. Roberta has an overall stereotype score of 49.9 (Ideal 50) with a higher presence of religion and gender stereotypes \cite{nadeem2020stereoset}.  

\subsubsection{XLNet - base - cased }
XlNet, a generalized autoregressive method, uses the best of both autoregressive language modeling and autoencoding language modeling tasks. Instead of using fixed left to right (predict the probability of next token to the right in a sentence) or right to left (predict the probability of next token to the left) autoregressive language modeling, XLNet uses all possible permutations where the model is used to predict token by changing the order of the prediction, hence capturing bidirectional context\cite{yang2019xlnet}.XlNet was trained on BookCorpus(11,038 unpublished books, 800 million words), English Wikipedia (excluding lists, tables, and headers, 2,500 million words) and  Giga5 (16GB text), ClueWeb 2012-B (English web pages collected between February 10, 2012 and May 10, 2012), Common Crawl (web crawl data) \cite{yang2019xlnet}. The stereotype score of XLNet is 54, with a higher presence of race and professional stereotypes \cite{nadeem2020stereoset}.
\subsubsection{GPT-2}
GPT-2 short for Generative Pre-trained Transformer 2 is a 12 layer decoder based transformer language model \cite{radford2019language}. GPT-2 is a purely autoregressive model and was trained on the classic language modeling task of predicting the probability distribution of text sequences \cite{radford2019language}. GPT-2 language model was trained on 40 GB of internet text collected from 8 million web pages. The web pages are outbound links on Reddit, which received at least 3 karma excluding Wikipedia text. The stereotype score of GPT-2 is 57, with a higher presence of religion and profession stereotypes. 

Since it is a purely autoregressive model to predict the next word while fine-tuning it for the downstream task such as classification, few changes have been made. Since the last token embedding has the contextual information in the left-to-right language modeling task, the embedding of the last hidden state is to be used for classification. The information regarding the last token needs to be provided in the config file for the model to use accordingly. "pad\_token\_id" needs to be specified for grabbing the last token that is not a pad token \footnote{\url{https://huggingface.co/transformers/model_doc/gpt2.html#gpt2forsequenceclassification}}. Hence, pad token should be added as a special token and should be specified to the tokenizer as an end of sequence token from where in the same procedure as done for the other autoencoding models follows.

\begin{figure}[]
    \centering
    \includegraphics[width=0.7\textwidth]{thesis/figures/Baseline_pipeline (2).png}
    \caption{feature-based approach pipeline}
    \label{fig:feature-based approach pipeline}
\end{figure}

\subsection{Feature-based approach}
In order to answer the second question and evaluate the fine-tuning-based approach, baselines have been trained using selected features. For this, Convolutional Neural Network (CNN) model for text classification has been trained using Glove (global vectors for word representation) \cite{pennington2014glove}, Fasttext  \cite{joulin2017bag} and flair  \cite{akbik2019flair} pre-trained word embeddings as features. A simple one-layer bi-directional Long Short-Term Memory Network (biLSTM) was also trained with randomly initialized weights. Popular Machine learning models used for classification, such as support vector machines with manually selected features, multinomial Naive Bayes (MNB) model with Term Frequency-Inverse Document Frequency (TF-IDF), and Bag of Words (BOW) have also been trained \cite{kowsari2019text}. Models which inherently support multi-label classification such as k-Nearest Neighbors (KNN) classifier, decision tree classifier, and random forest classifier, have also been experimented with TF-IDF and BOW features.

The baseline models have been trained on the train, validation set used for training the language models, and tested on the same test split with similar evaluation metrics for a fair comparison. The training pipeline followed for the feature-based approach is shown in the figure \ref{fig:feature-based approach pipeline}. During the pre-processing step, the input data has been cleaned (removing tokens that are not alphabetic), lowercased, and lemmatized to its base form with and without stop words using spacy\footnote{\url{https://spacy.io/}}. 


In the feature engineering step, the input data is vectorized using different features to feed it to the model. For CNN; Glove (6Billion tokens,100 dimensional \footnote{\url{https://nlp.stanford.edu/projects/glove/}}), fasttext (Wiki-news-300 dimensional \footnote{\url{https://fasttext.cc/docs/en/pretrained-vectors.html}}) and flair (4098 dimensional \footnote{\url{https://github.com/flairNLP/flair}}) pre-trained embeddings models were used as features. Keras tokenizer \footnote{\url{https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer}} has been used to turn the lemmatized text into tokens with corresponding token\_ids. These token ids would be used as index to access the corresponding token (word to index mapping). After turning the lemmatized input text sequences into sequences of ids, the sequences are padded to a fixed length. Then, the pre-trained word embeddings are extracted along with their corresponding word ids. Finally, a Keras embedding layer \footnote{\url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding}} of features corresponding to the word embedding model is created by extracting the word embeddings corresponding to the ids of the lemmatized input text sequence. The embedding layer has the shape of (v-vocab created by tokenizer, D-word embedding dimension). The same tokenizer is used, and the same methodology is followed for the three-word embedding models to have a fair comparison of results. For bidirectional LSTM, random word embeddings with randomly initialized weights with a similar configuration (embedding dimension, vocab size) used for glove is created. Coming to the machine learning models, selected lexical and syntactic features mainly from the paper \cite{recasens2013linguistic}  have been used along with TF-IDF and BOW features. The list of selected features can be seen in the table \ref{tab:features}. The table shows the feature name with value and type and a small description of the feature. There are two types of features, count-based features, which are purely based on count, and score-based features, which are calculated based on some formula. Coming to score-based features, flesch readability score \cite{flesch1948new} using textstat library \footnote{\url{https://textstat.readthedocs.io/en/latest/}} is used to determine the readability and test whether stereotypes are explicit/overt or subtle, subjectivity score using textblob \footnote{\url{https://textblob.readthedocs.io/en/dev/}} is used to determine whether stereotypes are subjective in nature, sentiment analysis using vader sentiment analysis library \footnote{\url{https://github.com/cjhutto/vaderSentiment}} with positive, negative, neutral scores have been used to determine sentiment of stereotypes. Average word length, max\_tf\_idf scores have also been used to gather general text statistics. 

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llll@{}}
\toprule
Id & Feature                  & Value                                   & Description                                                \\ \midrule
1  & Flesch\_score            & \textless{}Numeric, score\textgreater{} & Reading ease of the stereotype sample                      \\
2  & Subjectivity\_score      & \textless{}Numeric, score\textgreater{} & Subjectivity of stereotype sample                          \\
3  & Neg                      & \textless{}Numeric, score\textgreater{} & Negative score of stereotype sample                        \\
4  & Pos                      & \textless{}Numeric, score\textgreater{} & Positive score of stereotype sample                        \\
5  & Neu                      & \textless{}Numeric, score\textgreater{} & Neutral score of stereotype sample                         \\
6  & Num\_chars               & \textless{}Numeric, count\textgreater{} & Number of characters in stereotype sample                  \\
7  & Num\_words               & \textless{}Numeric, count\textgreater{} & Number of words in stereotype sample                       \\
8  & Avg\_word\_length        & \textless{}Numeric, score\textgreater{} & Average number of words in stereotype sample               \\
9  & Max\_tf\_idf\_feature    & \textless{}Numeric, score\textgreater{} & Maximum TF-IDF score of stereotype sample                  \\
10 & Assertive\_verbs\_count  & \textless{}Numeric, count\textgreater{} & Number of Assertive verbs in stereotype sample             \\
11 & Factive\_verb\_count     & \textless{}Numeric, count\textgreater{} & Number of factive verbs in stereotype sample               \\
12 & Hedges\_count            & \textless{}Numeric, count\textgreater{} & Number of hedges in stereotype samples                     \\
13 & Implicative\_verb\_count & \textless{}Numeric, count\textgreater{} & Number of implicative verbs in stereotype sample           \\
14 & SubjectivityClues\_lexicon\_count        & \textless{}Numeric, count\textgreater{} & Number of subjectivity clues in stereotype sample                           \\
15 & Bias\_word\_list\_01\_2018\_count        & \textless{}Numeric, count\textgreater{} & Number of bias words in stereotype sample                                   \\
16 & NNS\_count               & \textless{}Numeric, count\textgreater{} & Number of plural nouns in stereotype sample                \\
17 & NNPS\_count              & \textless{}Numeric, count\textgreater{} & Number of proper plural nouns in stereotype sample         \\
18 & DT\_count                & \textless{}Numeric, count\textgreater{} & Number of determiners in stereotype sample                 \\
19 & JJ\_count                & \textless{}Numeric, count\textgreater{} & Number of adjectives in stereotype sample                  \\
20 & JJS\_count               & \textless{}Numeric, count\textgreater{} & Number of superlative adjectives in stereotype sample      \\
21 & NN\_count                & \textless{}Numeric, count\textgreater{} & Number of nouns in stereotype sample                       \\
22 & Adverb\_count            & \textless{}Numeric, count\textgreater{} & Number of adverbs in stereotype sample                     \\
23 & NORP\_count                              & \textless{}Numeric, count\textgreater{} & Number of nationality or religious or political groups in stereotype sample \\
24 & PERSON\_count            & \textless{}Numeric, count\textgreater{} & Number of people, including fictional in stereotype sample \\
25 & GPE\_count               & \textless{}Numeric, count\textgreater{} & Number of countries, cities, states in stereotype sample   \\
26 & Characteristic\_terms\_ethnicity\_count  & \textless{}Numeric, count\textgreater{} & Number of characteristic ethnicity terms in stereotype sample               \\
27 & Characteristic\_terms\_profession\_count & \textless{}Numeric, count\textgreater{} & Number of characteristic profession terms in stereotype sample              \\
28 & Characteristic\_terms\_gender\_count     & \textless{}Numeric, count\textgreater{} & Number of characteristic gender terms in stereotype sample                  \\
29 & Characterisitc\_terms\_religion\_count   & \textless{}Numric, count\textgreater{}  & Number of characteristic religion terms in stereotype sample                \\ \bottomrule
\end{tabular}%
}
\caption{Manually selected features for SVM baseline}
\label{tab:features}
\end{table}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.7\textwidth]{thesis/figures/correlation_plot.png}
%     \caption{Correlation pair plot describing different features and their correlation}
%     \label{fig:feature-based approach pipeline}
% \end{figure}

Coming to the count-based features, lexicons gathered from the paper  \cite{recasens2013linguistic}  have been used as count-based features as described in the table \ref{tab:features}. The lexical features were used to test whether a sentence is biased or not. Coming to linguistic features [10-14 in table], Assertive verbs \cite{hooper1975assertive} are the verbs whose "complement clauses assert certainty of proposition"\cite{recasens2013linguistic}. E.g., verbs such as "point out" and "claim" cast doubt on the certainty of the proposition than verbs such as "say" and "state" \cite{recasens2013linguistic}. Factive verbs \cite{kiparsky1970progress} are the verbs that "pre-suppose the truth of their complement clause "\cite{recasens2013linguistic} e.g., "He realized" and "reveal" pre-suppose the truth rather than  "his stand" and "indicated". Hedges are the words used to avoid bold predictions or statements. E.g. words such as "may", "possibly" \cite{recasens2013linguistic}. Implicative verbs \cite{karttunen1971implicative} are the verbs "which implies truth or untruth of the complement depending on polarity"\cite{recasens2013linguistic}. E.g., verbs such as "murdered" with negative polarity, "coerced into accepting" with negative polarity than "killed" or "accepting." Overall, these linguistic features indicate bias in the proposition. Strong subjectivity words are extracted from subjectivity clues lexicon \cite{wilson2005recognizing} to determine the subjectivity involved in stereotypes. Parts of speech tags and Named entity recognition (NER) tags were created for the lemmatized input sentences using spacy library. Tags such as NNS (Plural nouns), NNPS (Proper plural nouns) were used to determine the generalization involved in the stereotype sample. Tags JJ(Adjective), JJS(superlative adjectives) were used to indicate attribute terms and superlative adjectives to indicate subjectivity. Tags DT (Determiner) were used due to the high presence of "The" with people found in the EDA report, which indicates generalization. Tag RB (adverb) to indicate generic sentences where frequency adverb (usually, typically, generally, sometimes, always) are used. Coming to NER tags, NORP (Nationalities or religious groups or political groups), GPE (Countries, cities, states) were used as a cue to detect ethnic groups. Finally, the top thousand characteristic terms based on scaled F-score (harmonic mean of precision and recall with scaling factor) from ethnicity, gender, profession, religion bias groups were extracted using scattertext \footnote{\url{https://github.com/JasonKessler/scattertext}} to indicate the presence of different bias types.

Coming to the model definition step, the CNN model was created and trained using Keras library \footnote{\url{https://keras.io/}}. The basic architecture of CNN model used is embedding layer followed by 1D convolutional layer with 128 filters to extract features with a kernel size of 3 followed by maxpooling1D with a pool size of 3 to extract the maximum value over the window. This setup is repeated three times, followed by a globalMaxpool1D to get overall important features and finally a dense output layer with an output size corresponding to a number of labels (in our case 7). Sigmoid activation function and binary cross-entropy loss function are used as we are dealing with multi-label classification. A simple one-layer biLSTM model was also experimented with to test how the use of random word embeddings rather than pre-trained embeddings would affect the evaluation metrics. Multinomial Naive Bayes (MNB) and Support Vector Machines (SVM), along with others, were created and trained using scikit learn library \footnote{\url{https://scikit-learn.org/stable/}}. The important step is that MNB and SVM don't inherently support multi-label classification. Hence MultiOutputClassifier \footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html#sklearn.multioutput.MultiOutputClassifier}} is used where one classifier is fit to each label to support multi-label classification following the binary relevance transformation method.

Coming to the training, in the case of CNN model with different word embedding features, the models were trained for 40 epochs, with a batch size of 128. Early stopping callback is used with patience set to 5.
Early stopping is a technique used to stop training by monitoring a metric (goal to minimize validation loss in our case) if there is no longer an improvement. Patience parameter is used to Waite for n epochs before stopping the training. Coming to machine learning models, the classifier models wrapped with multi-output classifier method along with other classifiers were fitted to train set (combination of train and validation splits). Finally, all the models are tested on the same test set used for testing the language models for a fair comparison.

\subsubsection{Brief description of models}
\textbf{------------------TBD------------------------------------------------}


Coming to the deep learning models, Convolutional Neural Network has been chosen to experiment with because CNN can be a much faster alternative to recurrent neural networks for tasks like text classification. 
\begin{enumerate}
    \item How does accuracy vary with non-contextual word embeddings (e.g. Glove, Word2Vec) to BERT word embedding?
    \item How does accuracy vary with baseline architecture (applying word embeddings to sentences and taking arithmetic average) to a fine-tuned BERT model ?
\end{enumerate}

    \begin{itemize}
        \item Lexicon - based approach : 
        \begin{itemize}
            \item Project into word embedding space and score each word based on its distance from woman, men ??
            \textbf{Refer }\cite{cryan2020detecting}
        \end{itemize}
        \item Brief description of  models and features
        \begin{itemize}
            \item SVM with selected lexicons + different features (toxicity, sentiment)
            \item Text-CNN with GLove,flair embedding
            \item Random embedding with GRU and LSTM 
        \end{itemize}
        \begin{itemize}
            \item Lexicons, POS and NER tags 
            \begin{itemize}
                \item Generalization : Plural nouns (NNS,NNPS), articles, article(the) + adjective(Refer to the whole group), determiner, quantifier (many,few, several...), frequency adverbs (Generic sentences - usually, typically, generally, sometimes, always)
                \item Subjectivity : strong subjectivity lexicon \cite{tangpersonalized}
                \item Syntactic cues : Syntactic negation (describing Stereotype-inconsistent )
            \end{itemize}
        \end{itemize}
        \item Why these model?
    \end{itemize}
\textbf{------------------TBD------------------------------------------------}
\section{Evaluation metrics} \label{Evaluation_metrics}
Evaluation of multi label classification problem is much complicated than traditional single label setting, as an example can be associated to multiple labels and the predictions of the learned model could be partially correct\cite{sorower2010literature}\cite{zhang2013review}. Hence, there arises a number of evaluation metrics that can be used to capture different notions. 
Broadly, the evaluation metrics used for multi-label classification can be categorized into two groups, namely example or sample-based metrics and label-based metrics \cite{zhang2013review}.
% \cite{tsoumakas2007multi}
\\

\\
\\
Formal definition, let $D$ be a multi-label dataset containing m multi-label samples $D = {(x_i,Y_i)|1\leq i \leq m}, (x_i \in X (instance space), Y_i (Label space) \in {0,1}^k)$ with a label set $L$, $|L|$=k. Let $S = {(x_i,Y_i) | ,1\leq i \leq p)}$ be the test set containing p multi label samples and h be a multi label classifier, $h(x_i)$ is a predicted label set of an instance $x$ \cite{zhang2013review}.

\subsection{example or sample-based metrics}
Example-based metrics evaluate the learned model on each test sample separately and return the mean over the test set\cite{zhang2013review}. The following sample or example-based metrics have been evaluated.
 
\subsubsection{Subset accuracy or Exact match ratio}
 Subset accuracy is also known as exact match ratio is a multi-label counterpart of traditional accuracy metric \cite{zhang2013review}. Subset accuracy can be defined as the fraction of correctly classified samples that exactly match the ground truth labels \cite{zhang2013review}. If the entire set of predictions matches true labels, then accuracy is one or else is zero. This is a harsh measure, as partially correct predictions are considered as completely incorrect predictions\cite{sorower2010literature} \cite{zhang2010multi}.  Overall subset accuracy is the average across all instances. 

 $$Subset Accuracy / Exact Match ratio = \frac{1}{p}\sum_{i =1}^{p}[h(x_i) == Y_i]$$

E.g.: 

y\_true = [0,1,0]; 
y\_ pred = [0,1,1];  
SubsetAccuracy/ExactMatchRatio = 0
 
\subsubsection{Hamming loss}

Hamming loss is the fraction of symmetric difference between the predicted label set $h(x_i)$ and ground truth label set $Y_i$ over the label space $q$ \cite{zhang2010multi}. Hamming loss evaluates the fraction of misclassified  instance-label pairs, i.e. prediction error(incorrect prediction) and missing error(relevant label not predicted) \cite{sorower2010literature}.  Overall hamming loss is the average across all instances.

 $$Hammingloss = \frac{1}{p}\sum_{i =1}^{p}\frac{1}{q}[h(x_i) \Delta Y_i]$$ \cite{zhang2010multi}
 

E.g.: 

y\_true = [0,1,0]; 
y\_ pred = [0,1,1];  
Hammingloss = 0.3333333333333333


\subsubsection{Hamming score / Accuracy}

 Accuracy is also known as Hamming score is the proportion of correctly predicted labels to the total number of predicted and actual positive labels. \cite{sorower2010literature}.  Overall accuracy is the average across all instances.

$$Hamming score / Accuracy = \frac{1}{p}\sum_{i =1}^{p}\frac{|Y_i \cap h(x_i)|}{|Y_i \cup h(x_i)|}$$ \cite{zhang2010multi}


E.g.: 

y\_true = [0,1,0]; 
y\_ pred = [0,1,1];  
Hamming score / Accuracy = 0.5

\subsubsection{Precision}

Precision can be defined as the proportion of correctly predicted positive labels to the total number of actual (true positive)
labels \cite{sorower2010literature}. Overall precision is the average across all instances.

$$Precision = \frac{1}{p}\sum_{i =1}^{p}\frac{|Y_i \cap h(x_i)|}{|h(x_i)|}$$ \cite{zhang2010multi}

E.g.: 

y\_true = [0,1,0]; 
y\_ pred = [0,1,1];  
Precision = 1.0

\subsubsection{Recall}

Recall can be defined as the proportion of correctly predicted positive label to the total number of predicted labels \cite{sorower2010literature}.

$$Recall = \frac{1}{p}\sum_{i =1}^{p}\frac{|Y_i \cap h(x_i)|}{|Y_i|}$$ \cite{zhang2010multi}

E.g.: 

y\_true = [0,1,0]; 
y\_ pred = [0,1,1];  
Recall = 0.5

\subsubsection{F-Measure}
F1-score is the harmonic mean of precision and recall. F1-score takes precision and recall scores into account to give an overall comprehensive score1\cite{sorower2010literature}.

$$F1-score = \frac{1}{p}\sum_{i =1}^{p}\frac{2|Y_i \cap h(x_i)|}{|Y_i| + |h(x_i)|}$$ \cite{zhang2010multi}

E.g.: 

y\_true = [0,1,0]; 
y\_ pred = [0,1,1]; 
F1-score =0.6666666666666666

\subsection{Label based metrics}
Label-based metrics evaluate the learned model by calculating metrics per label rather than per sample\cite{zhang2013review}. Label based metrics are based on \acrfull{tp}, \acrfull{fp}, \acrfull{tn},\acrfull{fn}. Here, true positive, false positive, true negative, false negative per class label are calculated by using one vs. rest strategy where the ground truth and prediction labels are used such that there are two levels (either belonging to the class or belonging to another) per class. Since the metric is calculated label-wise, evaluation metrics used for binary classification (precision, recall, f1-score, etc.)  can be directly used \cite{sorower2010literature}.  To determine the overall quality of the classification, there are two ways 
    
\subsubsection{Macro average (per-class average)}
Macro average score of the classifier is calculated by calculating the same measure (precision, recall, f1-score, etc.) for each individual class label separately and then average over all the classes \cite{sorower2010literature}. Macro average precision is the average precision calculated using one over each class. 

$$B_\mathrm{macro}(h) = \frac{1}{p}\sum_{i =1}^{p}B(TP_j,FP_j,TN_j,FN_j), (B\in \{Accuracy,precision,recall,f1score\})$$ \cite{zhang2010multi}

% \subsubsection{Macro average Area Under ROC score}
% Area under Receiver operating characteristics curve (ROC) is a model performance measurement score based on ROC curve. ROC curve is a graphical plot that gives a cumulative view of the true positive rate (TPR - [Y-axis]) and false positive rate (FPR - [X-axis]) at different thresholds in the case of binary classification. \cite{zhang2013review}. For multi-label, ROC is extended with one vs rest algorithm where TPR and FPR are plotted per class label \cite{aggarwal2012survey}. The area under ROC gives the cumulative measure of the entire area underneath the ROC curve, thus giving an overall cumulative measure of the performance of the model \cite{aggarwal2012survey}. The higher the score, the better the model is at distinguishing the positive and negative classes or avoid false classification\cite{sokolova2009systematic}. 

% $$AUC_macro = \frac{1}{p}\sum_{i =1}^{p} AUC_i$$
% \cite{zhang2010multi}

\subsubsection{Micro average (per-text average)}
On the other hand, the micro average score is calculated by calculating the measure globally over all the instances and class labels rather than measuring per individual labels.\cite{zhang2010multi}

$$B_\mathrm{micro}(h) = B\left(\sum_{i =1}^{p}TP_j,\sum_{i =1}^{p}FP_j,\sum_{i =1}^{p}TN_j,\sum_{i =1}^{p}FN_j\right), (B\in \{Accuracy,precision,recall,f1score\})$$ \cite{zhang2010multi}