\chapter{Conclusion}
The main goal of the thesis was to assess stereotypical social biases in text sequences. For this, two research questions were asked, namely, how far transfer learning using language models is useful and how will different pre-trained language models affect the detection of stereotypical social bias. To answer the first question, BERT, RoBERTa, Xlnet, and GPT-2 language models were trained, and out of all RoBERTa achieved the best results and GPT-2 had the lowest results. In order to evaluate the language models and answer the second question, different machine learning models with selected features and deep learning models with different pre-trained word embeddings were selected and trained. Random forests model achieved the highest results with k-Nearest Neighbor had the worst results. On an average, language models outperform machine learning models by 20\% and are significantly better than machine learning models. Hence, this answers the second question that transfer learning using pre-trained language models significantly impacts stereotypical social biases. The thesis work was carried out with the assumption that the dataset is validated. But, however, there are some pitfalls as stated in \cite{blodgett2021stereotyping}. As future work, this curated dataset can be validated to ensure more accuracy. Secondly, ranking-based metrics where the metrics are calculated based on the posterior probabilities could be used for better evaluation of models. 


% The goal of this thesis is two-fold. Firstly, to find how different pre-trained language models affect the detection of social bias and secondly, to find how far transfer learning (applying the real-world knowledge gained through language
% modeling) using language models is useful when detecting stereotypical social biases
% \section{Conclusion}
% The main goal of the thesis was to assess social stereotype present in text sequences. For this, two research questions were asked about how different language models impact the training and how far a fine-tuning based approach could assess stereotypes. To answer the first question, different language models were trained.   
% \section{Limitations}\label{limitations}
% The data used in this thesis were gathered from StereoSet \cite{nadeem2020stereoset} and CrowSpair \cite{nangia2020crows} and few more samples from social bias frames \cite{sap2019social}. The data, especially from StereoSet and CrowSpair had some pitfalls. This has also been addressed by the authors in the paper \cite{blodgett2021stereotyping}. Some StereoSet and CrowSpair stereotypical samples were not relevant or applicable to some named group. For example, in StereoSet gender domain, anti-stereotype sample which says "My grandfather is old" is neither stereotypical nor anti-stereotypical, and neither does it belong to any bias domain. Some gender-related samples from CrowSpair were purely descriptive, indicating and did not show any stereotypical attribute.  
% \begin{itemize}
%     \item Refer \cite{blodgett2021stereotyping} Aug 06,21 for more details 
%     \begin{itemize}
%         \item Meaningful stereotypes : Some StereoSet and CrowSpair stereotypical samples  were not relevant or applicable to some named group. E.g. In StereoSet, Gender domain Anti-stereo sample, \textit{ My grandfather is old} and stereo sample \textit{My grandfather is young} and in CS gender domain sample, stereo sample \textit{My mom spent all day cooking for thanksgiving} and anti-stereo sample \textit{My dad spent all day cooking for thanksgiving} which are neither relevant nor harmful to some named group. Particularly, samples belonging to gender domain face this pitfall. Some samples are subtle and subjective in nature e.g. He looked like a black rapper in drugs, which can be interpreted as black rapper do drugs (target -> association). Some samples gathered, especially from gender and religion bias types of SS and CS, are objective in nature E.g.: "The teacher is a Muslim. He teaches history and the kids love him." which makes it difficult 
%         \item Anti vs non-stereotypes : It is sometimes unclear whether the anti-stereotype sentence in the pair capture a descriptively true statement or simply negate the stereotypic statement. This is particularly true for Ethnicity bias. For E.g. 
%     \end{itemize}
%     \item The study was carried with the assumption that the datasets are validated. But due to the multidimensional nature of stereotypes and involved subjectivity, its not a straight forward task to gather and validate whether a sentence is stereotype or not.  
%     \item The datasets are gathered from crowdworkers from USA (80\% below 50) and stereotypes may vary based on countries. 
%     \item 
%     \item Some examples use first names to indicate the group which they belong to.
%     E.g. john ran into his old football friend
% \end{itemize}

% \section{Future work}
% \begin{itemize}
%     \item Evaluation of data with domain experts
%     \item technically, training language models with some layers frozen ??
%     \item Using different metrics such as ranking based metrics where the metrics is calculated based on the posterior probabilities. Look into different threshold.
%         \begin{itemize}
%             \item threshold calibration; using other threshold or using that which max suggested. 
%         \end{itemize}
%     \item Exploring multi label classification with class dependence
% \end{itemize}